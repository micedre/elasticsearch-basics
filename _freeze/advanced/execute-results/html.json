{
  "hash": "ba3f378b5bdfab88ac9f9a2ae1a577b7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Advanced Topics\"\n---\n\n## Introduction\n\nThis section covers advanced Elasticsearch concepts for production deployments, performance optimization, and specialized use cases.\n\n## Cluster Management\n\n### Cluster Health and Monitoring\n\n::: {#5eb9619f .cell execution_count=1}\n``` {.python .cell-code}\nfrom elasticsearch import Elasticsearch\n\nes = Elasticsearch(['http://localhost:9200'])\n\n# Detailed cluster health\nhealth = es.cluster.health()\n\nprint(f\"Cluster name: {health['cluster_name']}\")\nprint(f\"Status: {health['status']}\")  # green, yellow, red\nprint(f\"Active shards: {health['active_shards']}\")\nprint(f\"Relocating shards: {health['relocating_shards']}\")\nprint(f\"Unassigned shards: {health['unassigned_shards']}\")\n\n# Node stats\nnodes_stats = es.nodes.stats()\nfor node_id, stats in nodes_stats['nodes'].items():\n    print(f\"\\nNode: {stats['name']}\")\n    print(f\"  JVM heap used: {stats['jvm']['mem']['heap_used_percent']}%\")\n    print(f\"  CPU usage: {stats['os']['cpu']['percent']}%\")\n    print(f\"  Disk available: {stats['fs']['total']['available_in_bytes'] / (1024**3):.2f} GB\")\n```\n:::\n\n\n### Cluster State\n\nUnderstanding cluster health colors:\n\n- **Green**: All primary and replica shards are allocated\n- **Yellow**: All primary shards are allocated, but some replicas are not\n- **Red**: Some primary shards are not allocated\n\n::: {#efbcac8c .cell execution_count=2}\n``` {.python .cell-code}\n# Get detailed allocation info\nallocation = es.cat.allocation(format='json', v=True)\n\nfor alloc in allocation:\n    print(f\"Node: {alloc['node']}\")\n    print(f\"  Shards: {alloc['shards']}\")\n    print(f\"  Disk used: {alloc['disk.percent']}%\")\n```\n:::\n\n\n## Index Templates\n\nDefine templates for automatic index configuration:\n\n::: {#d0e102c5 .cell execution_count=3}\n``` {.python .cell-code}\n# Create index template\ntemplate = {\n    \"index_patterns\": [\"logs-*\"],  # Applies to indices matching pattern\n    \"template\": {\n        \"settings\": {\n            \"number_of_shards\": 2,\n            \"number_of_replicas\": 1,\n            \"refresh_interval\": \"30s\"\n        },\n        \"mappings\": {\n            \"properties\": {\n                \"timestamp\": {\"type\": \"date\"},\n                \"level\": {\"type\": \"keyword\"},\n                \"message\": {\"type\": \"text\"},\n                \"service\": {\"type\": \"keyword\"},\n                \"user_id\": {\"type\": \"keyword\"}\n            }\n        }\n    },\n    \"priority\": 100  # Higher priority templates override lower ones\n}\n\nes.indices.put_index_template(\n    name=\"logs-template\",\n    body=template\n)\n\nprint(\"Template created\")\n\n# Now any index matching \"logs-*\" will use this template\nes.indices.create(index=\"logs-2024-01\")  # Uses template automatically\n```\n:::\n\n\n## Index Lifecycle Management (ILM)\n\nAutomate index lifecycle with policies:\n\n::: {#aeff511f .cell execution_count=4}\n``` {.python .cell-code}\n# Define ILM policy\nilm_policy = {\n    \"policy\": {\n        \"phases\": {\n            \"hot\": {\n                \"actions\": {\n                    \"rollover\": {\n                        \"max_size\": \"50GB\",\n                        \"max_age\": \"30d\"\n                    }\n                }\n            },\n            \"warm\": {\n                \"min_age\": \"7d\",\n                \"actions\": {\n                    \"shrink\": {\n                        \"number_of_shards\": 1\n                    },\n                    \"forcemerge\": {\n                        \"max_num_segments\": 1\n                    }\n                }\n            },\n            \"cold\": {\n                \"min_age\": \"30d\",\n                \"actions\": {\n                    \"freeze\": {}\n                }\n            },\n            \"delete\": {\n                \"min_age\": \"90d\",\n                \"actions\": {\n                    \"delete\": {}\n                }\n            }\n        }\n    }\n}\n\n# Create ILM policy\nes.ilm.put_lifecycle(policy=\"logs-policy\", body=ilm_policy)\n\n# Apply to index template\ntemplate_with_ilm = {\n    \"index_patterns\": [\"logs-*\"],\n    \"template\": {\n        \"settings\": {\n            \"index.lifecycle.name\": \"logs-policy\",\n            \"index.lifecycle.rollover_alias\": \"logs\"\n        }\n    }\n}\n\nes.indices.put_index_template(\n    name=\"logs-ilm-template\",\n    body=template_with_ilm\n)\n```\n:::\n\n\n## Snapshots and Restore\n\n### Create Repository\n\n::: {#4f184116 .cell execution_count=5}\n``` {.python .cell-code}\n# Create filesystem repository (requires path.repo setting)\nrepository_config = {\n    \"type\": \"fs\",\n    \"settings\": {\n        \"location\": \"/backup/elasticsearch\",\n        \"compress\": True\n    }\n}\n\nes.snapshot.create_repository(\n    repository=\"backup_repo\",\n    body=repository_config\n)\n\n# For cloud storage (S3 example)\ns3_repository = {\n    \"type\": \"s3\",\n    \"settings\": {\n        \"bucket\": \"my-elasticsearch-backups\",\n        \"region\": \"us-east-1\",\n        \"compress\": True\n    }\n}\n```\n:::\n\n\n### Create Snapshot\n\n::: {#9395f269 .cell execution_count=6}\n``` {.python .cell-code}\n# Snapshot specific indices\nsnapshot_config = {\n    \"indices\": \"blog_posts,user_profiles\",\n    \"ignore_unavailable\": True,\n    \"include_global_state\": False\n}\n\nes.snapshot.create(\n    repository=\"backup_repo\",\n    snapshot=\"snapshot_2024_01\",\n    body=snapshot_config\n)\n\n# Check snapshot status\nstatus = es.snapshot.status(\n    repository=\"backup_repo\",\n    snapshot=\"snapshot_2024_01\"\n)\n\nprint(f\"Snapshot state: {status['snapshots'][0]['state']}\")\n```\n:::\n\n\n### Restore from Snapshot\n\n::: {#0164b479 .cell execution_count=7}\n``` {.python .cell-code}\n# Restore indices\nrestore_config = {\n    \"indices\": \"blog_posts\",\n    \"ignore_unavailable\": True,\n    \"include_global_state\": False,\n    \"rename_pattern\": \"(.+)\",\n    \"rename_replacement\": \"restored_$1\"  # Prefix with \"restored_\"\n}\n\nes.snapshot.restore(\n    repository=\"backup_repo\",\n    snapshot=\"snapshot_2024_01\",\n    body=restore_config\n)\n```\n:::\n\n\n## Performance Optimization\n\n### Bulk Indexing Best Practices\n\n::: {#c388dbe9 .cell execution_count=8}\n``` {.python .cell-code}\nfrom elasticsearch import helpers\n\ndef optimized_bulk_indexing(documents, index_name):\n    \"\"\"Optimized bulk indexing with best practices\"\"\"\n\n    # 1. Disable refresh during bulk indexing\n    es.indices.put_settings(\n        index=index_name,\n        body={\"index\": {\"refresh_interval\": \"-1\"}}\n    )\n\n    # 2. Increase replica count after indexing\n    es.indices.put_settings(\n        index=index_name,\n        body={\"index\": {\"number_of_replicas\": 0}}\n    )\n\n    try:\n        # 3. Use bulk helper with optimal chunk size\n        success, failed = helpers.bulk(\n            es,\n            documents,\n            chunk_size=500,  # Tune based on document size\n            request_timeout=60\n        )\n\n        print(f\"Indexed {success} documents\")\n\n    finally:\n        # 4. Re-enable refresh\n        es.indices.put_settings(\n            index=index_name,\n            body={\"index\": {\"refresh_interval\": \"1s\"}}\n        )\n\n        # 5. Restore replicas\n        es.indices.put_settings(\n            index=index_name,\n            body={\"index\": {\"number_of_replicas\": 1}}\n        )\n\n        # 6. Force merge to optimize segments\n        es.indices.forcemerge(\n            index=index_name,\n            max_num_segments=1\n        )\n\n# Example usage\n# documents = [{\"_index\": \"my_index\", \"_source\": {...}} for _ in range(10000)]\n# optimized_bulk_indexing(documents, \"my_index\")\n```\n:::\n\n\n### Query Optimization\n\n::: {#8e00b2ae .cell execution_count=9}\n``` {.python .cell-code}\n# 1. Use filter context (cached, no scoring)\noptimized_query = {\n    \"query\": {\n        \"bool\": {\n            \"must\": [\n                {\"match\": {\"content\": \"elasticsearch\"}}  # Scored\n            ],\n            \"filter\": [  # Cached, not scored\n                {\"term\": {\"status\": \"published\"}},\n                {\"range\": {\"published_date\": {\"gte\": \"2024-01-01\"}}}\n            ]\n        }\n    }\n}\n\n# 2. Use source filtering\noptimized_query[\"_source\"] = [\"title\", \"author\"]  # Only return needed fields\n\n# 3. Use size appropriately\noptimized_query[\"size\"] = 10  # Don't over-fetch\n\n# 4. Use request cache for aggregations\naggregation_query = {\n    \"query\": {\"match_all\": {}},\n    \"aggs\": {\n        \"popular_tags\": {\n            \"terms\": {\"field\": \"tags\"}\n        }\n    },\n    \"size\": 0\n}\n\n# Request cache is automatic for size=0 queries\n```\n:::\n\n\n### Shard Sizing\n\n::: {#db1a6451 .cell execution_count=10}\n``` {.python .cell-code}\n# Calculate optimal shard size\ndef calculate_shard_settings(total_data_gb, avg_doc_size_kb):\n    \"\"\"\n    Calculate optimal shard settings\n\n    Best practices:\n    - Shard size: 10-50 GB\n    - Shards per node: 20-25 per GB of heap\n    \"\"\"\n\n    # Target shard size (GB)\n    target_shard_size = 30\n\n    # Calculate number of shards\n    num_shards = max(1, int(total_data_gb / target_shard_size))\n\n    # Calculate expected shard size\n    shard_size = total_data_gb / num_shards\n\n    return {\n        \"number_of_shards\": num_shards,\n        \"expected_shard_size_gb\": shard_size,\n        \"recommendation\": \"Good\" if 10 <= shard_size <= 50 else \"Review\"\n    }\n\n# Example\nsettings = calculate_shard_settings(total_data_gb=100, avg_doc_size_kb=10)\nprint(settings)\n```\n:::\n\n\n## Custom Analyzers\n\n### Creating Custom Analyzers\n\n::: {#62ad0fae .cell execution_count=11}\n``` {.python .cell-code}\n# Advanced analyzer configuration\ncustom_analyzer_config = {\n    \"settings\": {\n        \"analysis\": {\n            \"char_filter\": {\n                \"quotes_filter\": {\n                    \"type\": \"mapping\",\n                    \"mappings\": [\n                        \"« => \\\"\",\n                        \"» => \\\"\"\n                    ]\n                }\n            },\n            \"tokenizer\": {\n                \"edge_ngram_tokenizer\": {\n                    \"type\": \"edge_ngram\",\n                    \"min_gram\": 2,\n                    \"max_gram\": 10,\n                    \"token_chars\": [\"letter\", \"digit\"]\n                }\n            },\n            \"filter\": {\n                \"custom_stop\": {\n                    \"type\": \"stop\",\n                    \"stopwords\": [\"the\", \"a\", \"an\"]\n                },\n                \"custom_stemmer\": {\n                    \"type\": \"stemmer\",\n                    \"language\": \"english\"\n                },\n                \"custom_synonym\": {\n                    \"type\": \"synonym\",\n                    \"synonyms\": [\n                        \"quick,fast,rapid\",\n                        \"laptop,notebook,computer\"\n                    ]\n                }\n            },\n            \"analyzer\": {\n                \"custom_search_analyzer\": {\n                    \"type\": \"custom\",\n                    \"char_filter\": [\"quotes_filter\"],\n                    \"tokenizer\": \"standard\",\n                    \"filter\": [\n                        \"lowercase\",\n                        \"custom_stop\",\n                        \"custom_synonym\",\n                        \"custom_stemmer\"\n                    ]\n                },\n                \"autocomplete_analyzer\": {\n                    \"type\": \"custom\",\n                    \"tokenizer\": \"edge_ngram_tokenizer\",\n                    \"filter\": [\"lowercase\"]\n                }\n            }\n        }\n    },\n    \"mappings\": {\n        \"properties\": {\n            \"title\": {\n                \"type\": \"text\",\n                \"analyzer\": \"custom_search_analyzer\"\n            },\n            \"title_autocomplete\": {\n                \"type\": \"text\",\n                \"analyzer\": \"autocomplete_analyzer\",\n                \"search_analyzer\": \"standard\"\n            }\n        }\n    }\n}\n\nes.indices.create(index=\"advanced_search\", body=custom_analyzer_config)\n```\n:::\n\n\n### Testing Analyzers\n\n::: {#ef716e7c .cell execution_count=12}\n``` {.python .cell-code}\n# Test how an analyzer processes text\nanalyze_request = {\n    \"analyzer\": \"custom_search_analyzer\",\n    \"text\": \"The Quick Brown Fox jumps over Laptops\"\n}\n\nresult = es.indices.analyze(index=\"advanced_search\", body=analyze_request)\n\nprint(\"Tokens produced:\")\nfor token in result['tokens']:\n    print(f\"  {token['token']} (position: {token['position']})\")\n```\n:::\n\n\n## Geo Queries\n\n### Geo Point Queries\n\n::: {#c75dd4ee .cell execution_count=13}\n``` {.python .cell-code}\n# Define mapping with geo_point\ngeo_mapping = {\n    \"mappings\": {\n        \"properties\": {\n            \"name\": {\"type\": \"text\"},\n            \"location\": {\"type\": \"geo_point\"}\n        }\n    }\n}\n\nes.indices.create(index=\"places\", body=geo_mapping)\n\n# Index documents with locations\nplaces = [\n    {\n        \"name\": \"Central Park\",\n        \"location\": {\"lat\": 40.785091, \"lon\": -73.968285}\n    },\n    {\n        \"name\": \"Times Square\",\n        \"location\": {\"lat\": 40.758896, \"lon\": -73.985130}\n    }\n]\n\nfor i, place in enumerate(places):\n    es.index(index=\"places\", id=i, document=place)\n\n# Search within distance\ngeo_query = {\n    \"query\": {\n        \"bool\": {\n            \"filter\": {\n                \"geo_distance\": {\n                    \"distance\": \"5km\",\n                    \"location\": {\n                        \"lat\": 40.7589,\n                        \"lon\": -73.9851\n                    }\n                }\n            }\n        }\n    }\n}\n\nresponse = es.search(index=\"places\", body=geo_query)\n\nfor hit in response['hits']['hits']:\n    print(f\"Found: {hit['_source']['name']}\")\n```\n:::\n\n\n## Scripting\n\n### Painless Scripts\n\n::: {#d6f8a108 .cell execution_count=14}\n``` {.python .cell-code}\n# Script in query\nscript_query = {\n    \"query\": {\n        \"script_score\": {\n            \"query\": {\"match_all\": {}},\n            \"script\": {\n                \"source\": \"Math.log(2 + doc['views'].value) * params.boost\",\n                \"params\": {\n                    \"boost\": 1.5\n                }\n            }\n        }\n    }\n}\n\n# Script in update\nscript_update = {\n    \"script\": {\n        \"source\": \"\"\"\n            if (ctx._source.views == null) {\n                ctx._source.views = 1;\n            } else {\n                ctx._source.views += params.increment;\n            }\n            ctx._source.last_updated = params.timestamp;\n        \"\"\",\n        \"params\": {\n            \"increment\": 1,\n            \"timestamp\": \"2024-01-15\"\n        }\n    }\n}\n\nes.update(index=\"blog_posts\", id=\"post_001\", body=script_update)\n```\n:::\n\n\n## Security Best Practices\n\n### Basic Security Checklist\n\n1. **Enable X-Pack Security** (built-in in recent versions)\n2. **Use HTTPS/TLS** for all communications\n3. **Implement Authentication**: API keys, Basic auth, or OAuth\n4. **Role-Based Access Control (RBAC)**\n5. **Audit Logging**\n6. **Network Security**: Firewall rules, VPC\n7. **Encryption at Rest**\n\n::: {#c4fab3f0 .cell execution_count=15}\n``` {.python .cell-code}\n# Connect with authentication\nfrom elasticsearch import Elasticsearch\n\n# Basic auth\nes = Elasticsearch(\n    ['https://localhost:9200'],\n    basic_auth=('username', 'password'),\n    ca_certs='/path/to/ca.crt'\n)\n\n# API key auth\nes = Elasticsearch(\n    ['https://localhost:9200'],\n    api_key=('api_key_id', 'api_key_secret')\n)\n\n# Cloud ID with auth\nes = Elasticsearch(\n    cloud_id='deployment:dXMtZWFzdC0xLmF3cy5mb3VuZC5pbyQ...',\n    basic_auth=('elastic', 'password')\n)\n```\n:::\n\n\n## Monitoring and Alerting\n\n### Key Metrics to Monitor\n\n::: {#6966af92 .cell execution_count=16}\n``` {.python .cell-code}\ndef get_cluster_metrics():\n    \"\"\"Get key cluster metrics for monitoring\"\"\"\n\n    metrics = {}\n\n    # Cluster health\n    health = es.cluster.health()\n    metrics['cluster_status'] = health['status']\n    metrics['active_shards'] = health['active_shards']\n    metrics['unassigned_shards'] = health['unassigned_shards']\n\n    # Node stats\n    nodes = es.nodes.stats()\n    total_jvm_heap = 0\n    total_cpu = 0\n    node_count = 0\n\n    for node_id, node in nodes['nodes'].items():\n        total_jvm_heap += node['jvm']['mem']['heap_used_percent']\n        total_cpu += node['os']['cpu']['percent']\n        node_count += 1\n\n    metrics['avg_jvm_heap_percent'] = total_jvm_heap / node_count\n    metrics['avg_cpu_percent'] = total_cpu / node_count\n\n    # Index stats\n    indices = es.indices.stats()\n    metrics['total_docs'] = indices['_all']['total']['docs']['count']\n    metrics['total_size_bytes'] = indices['_all']['total']['store']['size_in_bytes']\n\n    # Search performance\n    metrics['search_queries_total'] = indices['_all']['total']['search']['query_total']\n    metrics['search_time_ms'] = indices['_all']['total']['search']['query_time_in_millis']\n\n    return metrics\n\n# Monitor metrics\n# metrics = get_cluster_metrics()\n# print(metrics)\n```\n:::\n\n\n## Common Production Patterns\n\n### Time-Series Data Pattern\n\n::: {#922aaee2 .cell execution_count=17}\n``` {.python .cell-code}\n# Use index per time period\ndef create_daily_index(base_name, date):\n    \"\"\"Create daily index for logs/metrics\"\"\"\n    index_name = f\"{base_name}-{date.strftime('%Y.%m.%d')}\"\n\n    if not es.indices.exists(index=index_name):\n        es.indices.create(\n            index=index_name,\n            body={\n                \"settings\": {\n                    \"number_of_shards\": 3,\n                    \"number_of_replicas\": 1\n                }\n            }\n        )\n\n    return index_name\n\n# Use alias for querying\ndef setup_alias(base_name):\n    \"\"\"Setup rolling alias\"\"\"\n    es.indices.put_alias(\n        index=f\"{base_name}-*\",\n        name=f\"{base_name}-current\"\n    )\n\n# Query using alias\n# es.search(index=\"logs-current\", body=query)\n```\n:::\n\n\n:::{.callout-tip}\n## Production Checklist\n\nBefore going to production:\n\n- [ ] Enable security (authentication & encryption)\n- [ ] Set up monitoring and alerting\n- [ ] Configure snapshots/backups\n- [ ] Implement ILM policies\n- [ ] Size shards appropriately\n- [ ] Test disaster recovery procedures\n- [ ] Document cluster configuration\n- [ ] Set up proper logging\n- [ ] Configure resource limits\n- [ ] Plan capacity and scaling\n:::\n\n## Summary\n\nAdvanced Elasticsearch usage involves:\n\n1. **Cluster Management**: Monitoring health, nodes, and resources\n2. **Automation**: Index templates, ILM policies\n3. **Backup & Recovery**: Snapshots and restore procedures\n4. **Performance**: Optimization techniques for indexing and querying\n5. **Custom Analysis**: Specialized text processing\n6. **Geo Queries**: Location-based searching\n7. **Scripting**: Dynamic calculations and updates\n8. **Security**: Authentication, authorization, encryption\n9. **Monitoring**: Tracking key metrics and alerts\n10. **Production Patterns**: Best practices for real-world deployments\n\nCongratulations on completing this Elasticsearch training! You now have the knowledge to build, deploy, and manage production Elasticsearch clusters.\n\n",
    "supporting": [
      "advanced_files"
    ],
    "filters": [],
    "includes": {}
  }
}
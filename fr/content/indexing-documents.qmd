---
title: "Indexation de Documents"
format:
    html: default
    ipynb: default
---

## Introduction

L'indexation est le processus d'ajout de documents à Elasticsearch. Cette section couvre toutes les façons de créer, lire, mettre à jour et supprimer des documents, des opérations individuelles au traitement en masse et aux techniques avancées.

## Opérations sur Documents Individuels

### Indexer un Document

Il existe deux façons d'indexer un document : avec un ID généré automatiquement ou avec un ID personnalisé.

```{python}
#| echo: true
#| eval: false

from elasticsearch import Elasticsearch

es = Elasticsearch(['http://localhost:9200'])

# Index with auto-generated ID
response = es.index(
    index="blog_posts",
    document={
        "title": "Getting Started with Elasticsearch",
        "author": "John Doe",
        "content": "Elasticsearch is a powerful search engine...",
        "published_date": "2024-01-15",
        "tags": ["elasticsearch", "tutorial"],
        "views": 0
    }
)

print(f"Document indexed with ID: {response['_id']}")
print(f"Result: {response['result']}")  # 'created'

# Index with custom ID
response = es.index(
    index="blog_posts",
    id="post_001",
    document={
        "title": "Advanced Elasticsearch Queries",
        "author": "Jane Smith",
        "content": "Learn advanced query techniques...",
        "published_date": "2024-01-20",
        "tags": ["elasticsearch", "advanced"],
        "views": 150
    }
)

print(f"Document indexed with ID: {response['_id']}")  # 'post_001'
```

:::{.callout-note}
## ID Générés Automatiquement vs ID Personnalisés
- **Générés automatiquement** : Elasticsearch crée un ID unique (UUID encodé en base64)
- **ID personnalisé** : Vous fournissez l'ID (utile pour les clés naturelles comme email, nom d'utilisateur, etc.)
- **Idempotence** : Avec des ID personnalisés, la réindexation du même ID écrase le document
:::

### Récupérer un Document

Récupérer un document par son ID :

```{python}
#| echo: true
#| eval: false

# Get document by ID
doc = es.get(index="blog_posts", id="post_001")

print(f"Found: {doc['found']}")
print(f"Source: {doc['_source']}")
print(f"Version: {doc['_version']}")

# Access document fields
title = doc['_source']['title']
author = doc['_source']['author']
print(f"Title: {title}")
print(f"Author: {author}")
```

### Vérifier si un Document Existe

```{python}
#| echo: true
#| eval: false

# Check if document exists (efficient, doesn't return the document)
exists = es.exists(index="blog_posts", id="post_001")

if exists:
    print("Document exists")
else:
    print("Document not found")
```

### Mettre à Jour un Document

```{python}
#| echo: true
#| eval: false

# Partial update - only update specific fields
response = es.update(
    index="blog_posts",
    id="post_001",
    doc={
        "views": 200,
        "last_updated": "2024-01-25"
    }
)

print(f"Result: {response['result']}")  # 'updated'
print(f"New version: {response['_version']}")
```

### Supprimer un Document

```{python}
#| echo: true
#| eval: false

# Delete a document
response = es.delete(index="blog_posts", id="post_001")

print(f"Result: {response['result']}")  # 'deleted'

# Verify deletion
exists = es.exists(index="blog_posts", id="post_001")
print(f"Still exists: {exists}")  # False
```

## Opérations en Masse

Les opérations en masse vous permettent d'indexer, mettre à jour ou supprimer plusieurs documents en une seule requête, améliorant considérablement les performances.

### Utilisation Directe de l'API Bulk

```{python}
#| echo: true
#| eval: false

# Bulk API with mixed operations
bulk_body = [
    # Index operation
    {"index": {"_index": "blog_posts", "_id": "post_002"}},
    {"title": "Python Tutorial", "author": "Alice", "views": 50},

    # Another index operation
    {"index": {"_index": "blog_posts", "_id": "post_003"}},
    {"title": "JavaScript Guide", "author": "Bob", "views": 75},

    # Update operation
    {"update": {"_index": "blog_posts", "_id": "post_001"}},
    {"doc": {"views": 250}},

    # Delete operation
    {"delete": {"_index": "blog_posts", "_id": "post_old"}}
]

response = es.bulk(operations=bulk_body)

print(f"Took: {response['took']} ms")
print(f"Errors: {response['errors']}")

# Check individual results
for item in response['items']:
    for action, result in item.items():
        print(f"{action}: {result['_id']} - {result['result']}")
```

### Utilisation de helpers.bulk() (Recommandé)

La fonction `helpers.bulk()` est plus pratique et gère mieux les erreurs :

```{python}
#| echo: true
#| eval: false

from elasticsearch import helpers

# Prepare documents for bulk indexing
documents = [
    {
        "_index": "blog_posts",
        "_id": f"post_{i:03d}",
        "_source": {
            "title": f"Article {i}",
            "author": f"Author {i % 5}",
            "content": f"Content for article {i}...",
            "published_date": "2024-01-15",
            "tags": ["tutorial", "elasticsearch"],
            "views": i * 10
        }
    }
    for i in range(1, 101)  # Create 100 documents
]

# Bulk index
success, failed = helpers.bulk(
    es,
    documents,
    chunk_size=500,  # Number of docs per batch
    request_timeout=30
)

print(f"Successfully indexed: {success}")
print(f"Failed: {failed}")
```

### Indexation en Masse depuis un DataFrame

```{python}
#| echo: true
#| eval: false

import pandas as pd
from elasticsearch import helpers

# Create sample DataFrame
df = pd.DataFrame({
    'title': ['Post 1', 'Post 2', 'Post 3'],
    'author': ['Alice', 'Bob', 'Charlie'],
    'views': [100, 200, 150],
    'published_date': pd.date_range('2024-01-01', periods=3)
})

# Convert DataFrame to Elasticsearch bulk format
def dataframe_to_bulk(df, index_name):
    for idx, row in df.iterrows():
        yield {
            "_index": index_name,
            "_id": idx,  # Use DataFrame index as document ID
            "_source": row.to_dict()
        }

# Bulk index from DataFrame
success, failed = helpers.bulk(
    es,
    dataframe_to_bulk(df, "blog_posts")
)

print(f"Indexed {success} documents from DataFrame")
```

### Gestion des Erreurs dans les Opérations en Masse

```{python}
#| echo: true
#| eval: false

from elasticsearch import helpers

documents = [
    {"_index": "blog_posts", "_id": "1", "_source": {"title": "Valid"}},
    {"_index": "blog_posts", "_id": "2", "_source": {"invalid_field": "error"}},
    {"_index": "blog_posts", "_id": "3", "_source": {"title": "Also Valid"}},
]

# Bulk with error handling
errors = []

def handle_bulk_errors(success, error):
    errors.append(error)

success, failed = helpers.bulk(
    es,
    documents,
    raise_on_error=False,  # Don't stop on errors
    raise_on_exception=False,
    chunk_size=100
)

print(f"Successful: {success}")
print(f"Failed: {failed}")

# Log errors for debugging
if errors:
    print("Errors encountered:")
    for error in errors:
        print(f"  {error}")
```

### Conseils pour la Performance des Opérations en Masse

:::{.callout-tip}
## Optimisation de la Performance en Masse

1. **Taille des lots** : 500-1000 documents par lot est généralement optimal
2. **Désactiver le rafraîchissement** : Définir `refresh=False` pour accélérer l'indexation
3. **Augmenter le nombre de threads** : Utiliser le paramètre `thread_count` dans helpers.bulk()
4. **Surveiller les erreurs** : Toujours vérifier les documents ayant échoué
5. **Utiliser le streaming** : Les fonctions génératrices économisent la mémoire pour les grands ensembles de données

```python
# High-performance bulk indexing
success, failed = helpers.bulk(
    es,
    documents,
    chunk_size=1000,
    max_chunk_bytes=10485760,  # 10MB
    thread_count=4,
    refresh=False
)

# Refresh once after bulk indexing
es.indices.refresh(index="blog_posts")
```
:::

## Stratégies de Mise à Jour

### Mises à Jour Partielles

Mettre à jour uniquement des champs spécifiques sans remplacer l'ensemble du document :

```{python}
#| echo: true
#| eval: false

# Partial update with doc parameter
response = es.update(
    index="blog_posts",
    id="post_001",
    doc={
        "views": 300,
        "last_viewed": "2024-01-26"
    }
)

print(f"Updated, new version: {response['_version']}")
```

### Mises à Jour par Script

Utiliser des scripts Painless pour une logique de mise à jour complexe :

```{python}
#| echo: true
#| eval: false

# Increment views counter
response = es.update(
    index="blog_posts",
    id="post_001",
    script={
        "source": "ctx._source.views += params.increment",
        "params": {
            "increment": 10
        }
    }
)

print(f"Views incremented: {response['result']}")

# Conditional update with script
response = es.update(
    index="blog_posts",
    id="post_001",
    script={
        "source": """
            if (ctx._source.views > params.threshold) {
                ctx._source.popular = true;
                ctx._source.last_promoted = params.date;
            }
        """,
        "params": {
            "threshold": 1000,
            "date": "2024-01-26"
        }
    }
)

# Add to array field
response = es.update(
    index="blog_posts",
    id="post_001",
    script={
        "source": "ctx._source.tags.add(params.tag)",
        "params": {
            "tag": "popular"
        }
    }
)
```

### Upserts (Mise à Jour ou Insertion)

Créer un document s'il n'existe pas, ou le mettre à jour s'il existe :

```{python}
#| echo: true
#| eval: false

# Upsert: update if exists, insert if not
response = es.update(
    index="blog_posts",
    id="post_new",
    doc={
        "views": 10,
        "last_viewed": "2024-01-26"
    },
    upsert={
        "title": "New Post",
        "author": "Unknown",
        "content": "Default content",
        "views": 0,
        "created_at": "2024-01-26"
    }
)

print(f"Result: {response['result']}")  # 'created' or 'updated'

# Script-based upsert
response = es.update(
    index="blog_posts",
    id="post_counter",
    script={
        "source": "ctx._source.count += 1"
    },
    upsert={
        "count": 1
    }
)
```

### Mise à Jour par Requête

Mettre à jour plusieurs documents correspondant à une requête :

```{python}
#| echo: true
#| eval: false

# Update all posts by a specific author
response = es.update_by_query(
    index="blog_posts",
    body={
        "script": {
            "source": "ctx._source.verified = true",
            "lang": "painless"
        },
        "query": {
            "term": {
                "author": "John Doe"
            }
        }
    }
)

print(f"Updated: {response['updated']} documents")
print(f"Took: {response['took']} ms")

# Increment views for popular posts
response = es.update_by_query(
    index="blog_posts",
    body={
        "script": {
            "source": "ctx._source.views += params.bonus",
            "params": {
                "bonus": 50
            }
        },
        "query": {
            "range": {
                "views": {
                    "gte": 1000
                }
            }
        }
    }
)

print(f"Updated {response['updated']} popular posts")
```

### Suppression par Requête

Supprimer des documents correspondant à une requête :

```{python}
#| echo: true
#| eval: false

# Delete old posts
response = es.delete_by_query(
    index="blog_posts",
    body={
        "query": {
            "range": {
                "published_date": {
                    "lt": "2023-01-01"
                }
            }
        }
    }
)

print(f"Deleted: {response['deleted']} documents")
```

## Techniques d'Indexation Avancées

### Routage Personnalisé

Contrôler sur quel fragment un document est stocké :

```{python}
#| echo: true
#| eval: false

# Index with custom routing (all docs for same user on same shard)
response = es.index(
    index="blog_posts",
    id="post_user123_001",
    routing="user_123",  # Route by user ID
    document={
        "title": "My Post",
        "author": "user_123",
        "content": "..."
    }
)

# Must use same routing value when retrieving
doc = es.get(
    index="blog_posts",
    id="post_user123_001",
    routing="user_123"
)

# Search with routing (much faster)
result = es.search(
    index="blog_posts",
    routing="user_123",
    body={
        "query": {
            "match": {
                "content": "elasticsearch"
            }
        }
    }
)
```

:::{.callout-note}
## Quand Utiliser le Routage Personnalisé
- **Co-localisation** : Garder les documents liés ensemble (par ex., tous les articles d'un utilisateur)
- **Performance** : Rechercher uniquement dans des fragments spécifiques au lieu de tous
- **Cohérence** : Garantir que les documents sont sur le même fragment pour les jointures
:::

### Contrôle de Concurrence Optimiste

Empêcher les mises à jour simultanées de s'écraser mutuellement :

```{python}
#| echo: true
#| eval: false

# Get document with version
doc = es.get(index="blog_posts", id="post_001")
current_version = doc['_version']
seq_no = doc['_seq_no']
primary_term = doc['_primary_term']

print(f"Current version: {current_version}")

# Update with version check (old method)
try:
    response = es.index(
        index="blog_posts",
        id="post_001",
        if_seq_no=seq_no,
        if_primary_term=primary_term,
        document={
            "title": "Updated Title",
            "content": "..."
        }
    )
    print("Update successful")
except Exception as e:
    print(f"Version conflict: {e}")

# External versioning (for syncing with other systems)
response = es.index(
    index="blog_posts",
    id="post_001",
    version=5,  # Your external version number
    version_type="external",
    document={
        "title": "Updated Title"
    }
)
```

### Stratégies de Rafraîchissement

Contrôler quand les documents deviennent recherchables :

```{python}
#| echo: true
#| eval: false

# Immediate refresh (available immediately, but slower)
response = es.index(
    index="blog_posts",
    id="post_urgent",
    refresh=True,
    document={"title": "Breaking News"}
)

# Document is immediately searchable
result = es.search(
    index="blog_posts",
    body={"query": {"match": {"title": "Breaking News"}}}
)
print(f"Found: {result['hits']['total']['value']}")  # 1

# Wait for refresh (waits for next refresh interval)
response = es.index(
    index="blog_posts",
    id="post_normal",
    refresh="wait_for",
    document={"title": "Regular Post"}
)

# No refresh (fastest, default)
response = es.index(
    index="blog_posts",
    id="post_batch",
    refresh=False,  # or omit parameter
    document={"title": "Batch Post"}
)

# Manual refresh after bulk indexing
es.indices.refresh(index="blog_posts")
```

:::{.callout-warning}
## Compromis du Rafraîchissement
- **refresh=True** : Le plus lent, à utiliser uniquement si nécessaire (par ex., pour les tests)
- **refresh="wait_for"** : Plus lent, mais garantit la cohérence
- **refresh=False** : Le plus rapide, les documents apparaissent dans la seconde (intervalle de rafraîchissement par défaut)
- **Pour le bulk** : Toujours utiliser `refresh=False` et appeler `es.indices.refresh()` une fois à la fin
:::

### Alias d'Index

Utiliser des alias pour la réindexation sans temps d'arrêt :

```{python}
#| echo: true
#| eval: false

# Create initial index
es.indices.create(index="blog_posts_v1")

# Create alias pointing to it
es.indices.put_alias(index="blog_posts_v1", name="blog_posts")

# Application uses the alias
es.index(
    index="blog_posts",  # Uses blog_posts_v1
    document={"title": "Test"}
)

# Later: create new index with updated mapping
es.indices.create(
    index="blog_posts_v2",
    body={
        "mappings": {
            "properties": {
                "title": {"type": "text"},
                "new_field": {"type": "keyword"}  # New field
            }
        }
    }
)

# Reindex from v1 to v2
es.reindex(
    body={
        "source": {"index": "blog_posts_v1"},
        "dest": {"index": "blog_posts_v2"}
    }
)

# Atomic alias swap (zero downtime!)
es.indices.update_aliases(
    body={
        "actions": [
            {"remove": {"index": "blog_posts_v1", "alias": "blog_posts"}},
            {"add": {"index": "blog_posts_v2", "alias": "blog_posts"}}
        ]
    }
)

# Application continues using "blog_posts" alias, now pointing to v2
# Can safely delete v1 after verification
es.indices.delete(index="blog_posts_v1")
```

### Traitement par Pipeline

Prétraiter les documents avant l'indexation en utilisant des pipelines d'ingestion :

```{python}
#| echo: true
#| eval: false

# Create an ingest pipeline
es.ingest.put_pipeline(
    id="blog_post_pipeline",
    body={
        "description": "Process blog posts",
        "processors": [
            {
                "set": {
                    "field": "indexed_at",
                    "value": "{{_ingest.timestamp}}"
                }
            },
            {
                "lowercase": {
                    "field": "author"
                }
            },
            {
                "split": {
                    "field": "tags",
                    "separator": ","
                }
            }
        ]
    }
)

# Index with pipeline
response = es.index(
    index="blog_posts",
    id="post_pipeline",
    pipeline="blog_post_pipeline",
    document={
        "title": "Test Post",
        "author": "JOHN DOE",  # Will be lowercased
        "tags": "elasticsearch,python,tutorial"  # Will be split into array
    }
)

# Verify processing
doc = es.get(index="blog_posts", id="post_pipeline")
print(f"Author: {doc['_source']['author']}")  # "john doe"
print(f"Tags: {doc['_source']['tags']}")  # ["elasticsearch", "python", "tutorial"]
print(f"Indexed at: {doc['_source']['indexed_at']}")
```

## Bonnes Pratiques

:::{.callout-tip}
## Bonnes Pratiques d'Indexation

1. **Utiliser les Opérations en Masse** : Toujours regrouper plusieurs documents (500-1000 par lot)
2. **Gérer les Erreurs** : Vérifier la réponse bulk pour les documents ayant échoué
3. **Désactiver le Rafraîchissement** : Utiliser `refresh=False` pour le bulk, rafraîchir une fois à la fin
4. **Utiliser des Générateurs** : Économiser la mémoire lors de l'indexation de grands ensembles de données
5. **Surveiller les Performances** : Suivre le taux d'indexation et les erreurs
6. **Utiliser des Alias** : Permettre la réindexation sans temps d'arrêt
7. **Contrôle de Version** : Utiliser seq_no/primary_term pour les mises à jour simultanées
8. **Traitement par Pipeline** : Déléguer la transformation des données à Elasticsearch
9. **Routage Personnalisé** : Co-localiser les documents liés pour de meilleures performances
10. **Opérations Asynchrones** : Utiliser le client Python async pour les charges de travail liées aux E/S

```python
# Production-ready bulk indexing example
from elasticsearch import helpers
import logging

logging.basicConfig(level=logging.INFO)

def index_documents(es, documents, index_name):
    """Robust bulk indexing with error handling"""

    def generate_actions():
        for doc in documents:
            yield {
                "_index": index_name,
                "_id": doc.get("id"),
                "_source": doc
            }

    success = 0
    failed = 0

    for ok, response in helpers.streaming_bulk(
        es,
        generate_actions(),
        chunk_size=500,
        max_retries=3,
        raise_on_error=False
    ):
        if ok:
            success += 1
        else:
            failed += 1
            logging.error(f"Failed to index document: {response}")

        # Log progress every 1000 documents
        if (success + failed) % 1000 == 0:
            logging.info(f"Indexed: {success}, Failed: {failed}")

    # Refresh index
    es.indices.refresh(index=index_name)

    return success, failed
```
:::

## Résumé

Concepts clés de l'indexation :

1. **Opérations Individuelles** : index, get, update, delete pour les documents individuels
2. **Opérations en Masse** : Utiliser helpers.bulk() pour un traitement par lots efficace
3. **Stratégies de Mise à Jour** : Mises à jour partielles, scripts, upserts, update_by_query
4. **Techniques Avancées** : Routage, gestion des versions, contrôle du rafraîchissement, alias, pipelines

Elasticsearch fournit des options d'indexation flexibles pour chaque cas d'usage :
- Indexation en temps réel avec rafraîchissement immédiat
- Traitement par lots à haut débit
- Logique de mise à jour complexe avec des scripts
- Migrations de schéma sans temps d'arrêt avec des alias

:::{.callout-tip}
## Prochaines Étapes
Maintenant que vous comprenez l'indexation, explorez la section Langages de Requête pour apprendre à rechercher et récupérer vos documents !
:::

---

## Télécharger le Notebook

:::{.callout-note}
## Jupyter Notebook
Téléchargez cette section sous forme de notebook Jupyter interactif pour exécuter les exemples sur votre propre machine.

[Télécharger indexing-documents.ipynb](indexing-documents.ipynb){.btn .btn-primary download="indexing-documents.ipynb"}
:::
